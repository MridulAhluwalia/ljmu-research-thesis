{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c3100b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "import torch\n",
    "import config\n",
    "import torch.optim as optim\n",
    "import ignite.distributed as idist\n",
    "\n",
    "from train import get_loader\n",
    "from model import Generator, Discriminator\n",
    "from utils import gradient_penalty\n",
    "from ignite.engine import Engine\n",
    "from ignite.metrics import FID, InceptionScore, RunningAverage, SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52e474ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(\n",
    "    config.Z_DIM, config.IN_CHANNELS, img_channels=config.CHANNELS_IMG\n",
    ").to(idist.device())\n",
    "\n",
    "critic = Discriminator(\n",
    "    config.IN_CHANNELS, img_channels=config.CHANNELS_IMG\n",
    ").to(idist.device())\n",
    "\n",
    "# initialize optimizers and scalers for FP16 training\n",
    "opt_gen = optim.Adam(\n",
    "    gen.parameters(), lr=config.LEARNING_RATE, betas=(0.0, 0.99)\n",
    ")\n",
    "opt_critic = optim.Adam(\n",
    "    critic.parameters(), lr=config.LEARNING_RATE, betas=(0.0, 0.99)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33c39655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:116: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    }
   ],
   "source": [
    "scaler_critic = torch.cuda.amp.GradScaler()\n",
    "scaler_gen = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4941fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_fn(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "# @trainer.on(Events.STARTED)\n",
    "def init_weights():\n",
    "    gen.apply(initialize_fn)\n",
    "    critic.apply(initialize_fn)\n",
    "\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def store_losses(engine):\n",
    "    o = engine.state.output\n",
    "    G_losses.append(o[\"Loss_G\"])\n",
    "    D_losses.append(o[\"Loss_D\"])\n",
    "\n",
    "\n",
    "img_list = []\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def store_images(engine):\n",
    "    with torch.no_grad():\n",
    "        fake = gen(fixed_noise).cpu()\n",
    "    img_list.append(fake)\n",
    "\n",
    "    plt.title(\"Fake Images\")\n",
    "    plt.imshow(\n",
    "        np.transpose(\n",
    "            vutils.make_grid(img_list[-1], padding=2, normalize=True).cpu(),\n",
    "            (1, 2, 0),\n",
    "        )\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3ba035a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /Users/mac/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b112bd71e744508bb810a381715f9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/104M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fid_metric = FID(device=idist.device())\n",
    "is_metric = InceptionScore(\n",
    "    device=idist.device(), output_transform=lambda x: x[0]\n",
    ")\n",
    "ssim_metric = SSIM(data_range=1.0, device=idist.device())\n",
    "\n",
    "\n",
    "def interpolate(batch):\n",
    "    arr = []\n",
    "    for img in batch:\n",
    "        pil_img = transforms.ToPILImage()(img)\n",
    "        resized_img = pil_img.resize((299, 299), Image.BILINEAR)\n",
    "        arr.append(transforms.ToTensor()(resized_img))\n",
    "    return torch.stack(arr)\n",
    "\n",
    "\n",
    "def evaluation_step(engine, batch):\n",
    "    global gReal, gFake\n",
    "    gReal, _ = batch\n",
    "    gReal = gReal.to(idist.device())\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(batch_size, latent_dim, 1, 1, device=idist.device())\n",
    "        gFake = netG(noise)\n",
    "        fake = interpolate(gFake)\n",
    "        real = interpolate(gReal)\n",
    "        return fake, real\n",
    "\n",
    "\n",
    "evaluator = Engine(evaluation_step)\n",
    "fid_metric.attach(evaluator, \"fid\")\n",
    "is_metric.attach(evaluator, \"is\")\n",
    "# ssim_metric.attach(evaluator, \"ssim\")\n",
    "\n",
    "previous_model = None\n",
    "\n",
    "fid_values = []\n",
    "is_values = []\n",
    "ssim_values = []\n",
    "\n",
    "# @trainer.on(Events.EPOCH_COMPLETED)\n",
    "def save_model(engine):\n",
    "    global previous_model, MODEL_PATH\n",
    "    if engine.state.epoch < save_threshold:\n",
    "        return\n",
    "    if fid_values and (fid_values[-1] == min(fid_values[-(save_threshold-1):])):\n",
    "        print(\"Saving new model\")\n",
    "        MODEL_PATH = f\"./models/{model_name}_n_epoch_{engine.state.epoch}_G_losses_{G_losses[-1]:4f}_D_losses_{D_losses[-1]:4f}_fid_{fid_values[-1]:4f}_is_{is_values[-1]:4f}_ssim_{ssim_values[-1]:4f}.pth\"\n",
    "        torch.save(netG, MODEL_PATH)\n",
    "        if previous_model:\n",
    "            print(\"Removing previous model\")\n",
    "            os.remove(previous_model)\n",
    "        previous_model = MODEL_PATH\n",
    "\n",
    "\n",
    "# @trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(engine):\n",
    "    global gReal, gFake\n",
    "    evaluator.run(test_dataloader, max_epochs=1)\n",
    "    metrics = evaluator.state.metrics\n",
    "    fid_score = metrics[\"fid\"]\n",
    "    is_score = metrics[\"is\"]\n",
    "    fid_values.append(fid_score)\n",
    "    is_values.append(is_score)\n",
    "    \n",
    "    ssim_metric = SSIM(data_range=1.0, device=idist.device())\n",
    "    ssim_metric.update((gFake, gReal))\n",
    "    ssim_score = float(ssim_metric.compute())\n",
    "    ssim_values.append(ssim_score)\n",
    "    \n",
    "    print(f\"Epoch [{engine.state.epoch}/{n_epoch}] Metric Scores\")\n",
    "    print(f\"*       FID : {fid_score:4f}\")\n",
    "    print(f\"*        IS : {is_score:4f}\")\n",
    "    print(f\"*      SSIM : {ssim_score:4f}\")\n",
    "    print(f\"*  G_losses : {G_losses[-1]:4f}\")\n",
    "    print(f\"*  D_losses : {D_losses[-1]:4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "250faefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "RunningAverage(output_transform=lambda x: x[\"Loss_G\"]).attach(trainer, \"Loss_G\")\n",
    "RunningAverage(output_transform=lambda x: x[\"Loss_D\"]).attach(trainer, \"Loss_D\")\n",
    "\n",
    "ProgressBar().attach(trainer, metric_names=[\"Loss_G\", \"Loss_D\"])\n",
    "ProgressBar().attach(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4aa4734",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown backend 'nccl'. Available backends: ('gloo',)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/69/xtc57wnx0_lgkbcpdv4z6vc80000gn/T/ipykernel_50722/2207572407.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPROGRESSIVE_EPOCHS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0midist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nccl\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/ignite/distributed/launcher.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, backend, nproc_per_node, nnodes, node_rank, master_addr, master_port, init_method, **spawn_kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavailable_backends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unknown backend '{backend}'. Available backends: {idist.available_backends()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"nproc_per_node\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nnodes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"node_rank\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"master_addr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"master_port\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown backend 'nccl'. Available backends: ('gloo',)"
     ]
    }
   ],
   "source": [
    "def training_step(engine, batch):\n",
    "    global alpha, step, dataset\n",
    "    gen.train()\n",
    "    critic.train()\n",
    "    \n",
    "    real, _ = batch\n",
    "    \n",
    "    real = real.to(idist.device())\n",
    "    noise = torch.randn(batch_size, config.Z_DIM, 1, 1, device=idist.device())\n",
    "    \n",
    "    with torch.cuda.amp.autocast():\n",
    "        fake = gen(noise, alpha, step)\n",
    "        critic_real = critic(real, alpha, step)\n",
    "        critic_fake = critic(fake.detach(), alpha, step)\n",
    "        gp = gradient_penalty(\n",
    "            critic, real, fake, alpha, step, device=idist.device()\n",
    "        )\n",
    "        loss_critic = (\n",
    "            -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
    "            + config.LAMBDA_GP * gp\n",
    "            + (0.001 * torch.mean(critic_real ** 2))\n",
    "        )\n",
    "\n",
    "    opt_critic.zero_grad()\n",
    "    scaler_critic.scale(loss_critic).backward()\n",
    "    scaler_critic.step(opt_critic)\n",
    "    scaler_critic.update()\n",
    "\n",
    "    # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
    "    with torch.cuda.amp.autocast():\n",
    "        gen_fake = critic(fake, alpha, step)\n",
    "        loss_gen = -torch.mean(gen_fake)\n",
    "\n",
    "    opt_gen.zero_grad()\n",
    "    scaler_gen.scale(loss_gen).backward()\n",
    "    scaler_gen.step(opt_gen)\n",
    "    scaler_gen.update()\n",
    "\n",
    "    # Update alpha and ensure less than 1\n",
    "    alpha += batch_size / (\n",
    "        (config.PROGRESSIVE_EPOCHS[step] * 0.5) * len(dataset)\n",
    "    )\n",
    "    alpha = min(alpha, 1)\n",
    "\n",
    "    \n",
    "trainer = Engine(training_step)\n",
    "\n",
    "def training(*args):\n",
    "    global alpha, step, dataset\n",
    "    start_step = int(log2(config.START_TRAIN_AT_IMG_SIZE / 4))\n",
    "    for step in range(start_step, 7):\n",
    "        alpha = 1e-5\n",
    "        image_size = 4 * 2 ** step\n",
    "        loader, dataset = get_loader(image_size)\n",
    "        trainer.run(loader, max_epochs=config.PROGRESSIVE_EPOCHS[step])\n",
    "\n",
    "with idist.Parallel(backend=\"nccl\") as parallel:\n",
    "    parallel.run(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d48436d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
