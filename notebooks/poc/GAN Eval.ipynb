{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0551d1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.DCGAN'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/69/xtc57wnx0_lgkbcpdv4z6vc80000gn/T/ipykernel_38392/3483631224.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDCGAN\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mignite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandlers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProgressBar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mignite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEngine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src.DCGAN'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import PIL.Image as Image\n",
    "\n",
    "import ignite\n",
    "import ignite.distributed as idist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from src.DCGAN import Discriminator, Generator, get_noise\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import FID, InceptionScore, RunningAverage, SSIM\n",
    "\n",
    "from torchsummary import summary\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ddfb9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "n_channels = 3\n",
    "image_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad7c921c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 64, 64]           6,272\n",
      "         LeakyReLU-2          [-1, 128, 64, 64]               0\n",
      "            Conv2d-3          [-1, 256, 32, 32]         524,544\n",
      "       BatchNorm2d-4          [-1, 256, 32, 32]             512\n",
      "         LeakyReLU-5          [-1, 256, 32, 32]               0\n",
      "            Conv2d-6          [-1, 512, 16, 16]       2,097,664\n",
      "       BatchNorm2d-7          [-1, 512, 16, 16]           1,024\n",
      "         LeakyReLU-8          [-1, 512, 16, 16]               0\n",
      "            Conv2d-9           [-1, 1024, 8, 8]       8,389,632\n",
      "      BatchNorm2d-10           [-1, 1024, 8, 8]           2,048\n",
      "        LeakyReLU-11           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-12           [-1, 2048, 4, 4]      33,556,480\n",
      "      BatchNorm2d-13           [-1, 2048, 4, 4]           4,096\n",
      "        LeakyReLU-14           [-1, 2048, 4, 4]               0\n",
      "           Conv2d-15              [-1, 1, 1, 1]          32,769\n",
      "          Sigmoid-16              [-1, 1, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 44,615,041\n",
      "Trainable params: 44,615,041\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 19.25\n",
      "Params size (MB): 170.19\n",
      "Estimated Total Size (MB): 189.63\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    DCGAN Discriminator\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        features_dim=128,\n",
    "        img_channels=3,\n",
    "        kernel_size=4,\n",
    "        stride=2,\n",
    "        padding=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Discriminator Class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img_channels: int\n",
    "            number if channels in image\n",
    "        features_dim: int\n",
    "            inner hidden dimension\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        none\n",
    "        \"\"\"\n",
    "\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            # img_channels x 64 x 64\n",
    "            nn.Conv2d(img_channels, features_dim, kernel_size, stride, padding),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # features_dim x 32 x 32\n",
    "            nn.Conv2d(\n",
    "                features_dim, features_dim * 2, kernel_size, stride, padding\n",
    "            ),\n",
    "            nn.BatchNorm2d(features_dim * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # (features_dim * 2) x 16 x 16\n",
    "            nn.Conv2d(\n",
    "                features_dim * 2, features_dim * 4, kernel_size, stride, padding\n",
    "            ),\n",
    "            nn.BatchNorm2d(features_dim * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # (features_dim * 4) x 8 x 8\n",
    "            nn.Conv2d(\n",
    "                features_dim * 4, features_dim * 8, kernel_size, stride, padding\n",
    "            ),\n",
    "            nn.BatchNorm2d(features_dim * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # (features_dim * 8) x 4 x 4\n",
    "            nn.Conv2d(\n",
    "                features_dim * 8, features_dim * 16, kernel_size, stride, padding\n",
    "            ),\n",
    "            nn.BatchNorm2d(features_dim * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # (features_dim * 16) x 2 x 2\n",
    "            nn.Conv2d(features_dim * 16, 1, kernel_size, stride=1, padding=0),\n",
    "            # 1 x 1 x 1\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Function for completing a forward pass of the discriminator: Given an image tensor,\n",
    "        returns a 1-dimension tensor representing fake/real.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        image: tensor\n",
    "            a flattened image tensor with dimension (im_dim)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        _: tensor\n",
    "            real or fake\n",
    "        \"\"\"\n",
    "        return self.net(image)\n",
    "\n",
    "image_size = 128\n",
    "netD = idist.auto_model(Discriminator())\n",
    "summary(netD, (n_channels, image_size, image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77266b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ConvTranspose2d-1           [-1, 2048, 4, 4]       8,390,656\n",
      "       BatchNorm2d-2           [-1, 2048, 4, 4]           4,096\n",
      "              ReLU-3           [-1, 2048, 4, 4]               0\n",
      "   ConvTranspose2d-4           [-1, 1024, 8, 8]      33,555,456\n",
      "       BatchNorm2d-5           [-1, 1024, 8, 8]           2,048\n",
      "              ReLU-6           [-1, 1024, 8, 8]               0\n",
      "   ConvTranspose2d-7          [-1, 512, 16, 16]       8,389,120\n",
      "       BatchNorm2d-8          [-1, 512, 16, 16]           1,024\n",
      "              ReLU-9          [-1, 512, 16, 16]               0\n",
      "  ConvTranspose2d-10          [-1, 256, 32, 32]       2,097,408\n",
      "      BatchNorm2d-11          [-1, 256, 32, 32]             512\n",
      "             ReLU-12          [-1, 256, 32, 32]               0\n",
      "  ConvTranspose2d-13          [-1, 128, 64, 64]         524,416\n",
      "      BatchNorm2d-14          [-1, 128, 64, 64]             256\n",
      "             ReLU-15          [-1, 128, 64, 64]               0\n",
      "  ConvTranspose2d-16          [-1, 3, 128, 128]           6,147\n",
      "             Tanh-17          [-1, 3, 128, 128]               0\n",
      "================================================================\n",
      "Total params: 52,971,139\n",
      "Trainable params: 52,971,139\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 24.00\n",
      "Params size (MB): 202.07\n",
      "Estimated Total Size (MB): 226.07\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    DCGAN Generator\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        z_dim,\n",
    "        features_dim=128,\n",
    "        img_channels=3,\n",
    "        kernel_size=4,\n",
    "        stride=2,\n",
    "        padding=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generator Class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        z_dim: int\n",
    "            the dimension of the noise vector\n",
    "        img_channels: int\n",
    "            number if channels in image\n",
    "        features_dim: int\n",
    "            inner hidden dimension\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        none\n",
    "        \"\"\"\n",
    "\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            # z_dim x 1 x 1\n",
    "            nn.ConvTranspose2d(\n",
    "                z_dim, features_dim * 16, kernel_size, stride=1, padding=0\n",
    "            ),\n",
    "            nn.BatchNorm2d(features_dim * 16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # (features_dim * 16) x 4 x 4\n",
    "            nn.ConvTranspose2d(\n",
    "                features_dim * 16,\n",
    "                features_dim * 8,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "            ),\n",
    "            nn.BatchNorm2d(features_dim * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # (features_dim * 8) x 8 x 8\n",
    "            nn.ConvTranspose2d(\n",
    "                features_dim * 8, features_dim * 4, kernel_size, stride, padding\n",
    "            ),\n",
    "            nn.BatchNorm2d(features_dim * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # (features_dim * 4) x 16 x 16\n",
    "            nn.ConvTranspose2d(\n",
    "                features_dim * 4, features_dim * 2, kernel_size, stride, padding\n",
    "            ),\n",
    "            nn.BatchNorm2d(features_dim * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # (features_dim * 2) x 32 x 32\n",
    "            nn.ConvTranspose2d(\n",
    "                features_dim * 2, features_dim, kernel_size, stride, padding\n",
    "            ),\n",
    "            nn.BatchNorm2d(features_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # (features_dim) x 64 x 64\n",
    "            nn.ConvTranspose2d(\n",
    "                features_dim, img_channels, kernel_size, stride, padding\n",
    "            ),\n",
    "            # img_channels x 128 x 128\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Function for completing a forward pass of the generator: Given a noise tensor,\n",
    "        returns generated images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        noise: tensor\n",
    "            a noise tensor with dimensions (n_samples, z_dim, 1, 1)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        _: image\n",
    "            image after a forward pass\n",
    "        \"\"\"\n",
    "        return self.net(image)\n",
    "\n",
    "latent_dim = 256\n",
    "netG = idist.auto_model(Generator(latent_dim))\n",
    "summary(netG, (latent_dim, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41df3c93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e445e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c80681f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Discriminator and Generator implementation from DCGAN paper\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            # input: N x channels_img x 64 x 64\n",
    "            nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
    "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Input: N x channels_noise x 1 x 1\n",
    "            self._block(channels_noise, features_g * 16, 4, 1, 0),  # img: 4x4\n",
    "            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
    "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
    "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
    "            nn.ConvTranspose2d(\n",
    "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            # Output: N x channels_img x 64 x 64\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def initialize_weights(model):\n",
    "    # Initializes weights according to the DCGAN paper\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "\n",
    "\n",
    "\n",
    "N, in_channels, H, W = 8, 3, 64, 64\n",
    "noise_dim = 100\n",
    "x = torch.randn((N, in_channels, H, W))\n",
    "disc = Discriminator(in_channels, 8)\n",
    "assert disc(x).shape == (N, 1, 1, 1), \"Discriminator test failed\"\n",
    "gen = Generator(noise_dim, in_channels, 8)\n",
    "z = torch.randn((N, noise_dim, 1, 1))\n",
    "assert gen(z).shape == (N, in_channels, H, W), \"Generator test failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b1209b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 1, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b318104e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc(x).reshape(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afcc89d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 64, 64])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(z).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a9e9ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([98304])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(z).reshape(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51b114db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Discriminator test failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/69/xtc57wnx0_lgkbcpdv4z6vc80000gn/T/ipykernel_38093/2712276209.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdisc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mdisc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Discriminator test failed\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Discriminator test failed"
     ]
    }
   ],
   "source": [
    "N, in_channels, H, W = 64, 3, 64, 64\n",
    "noise_dim = 100\n",
    "x = torch.randn((N, in_channels, H, W))\n",
    "disc = Critic(in_channels)\n",
    "assert disc(x).shape == (N, 1, 1, 1), \"Discriminator test failed\"\n",
    "gen = Generator(noise_dim, in_channels)\n",
    "z = torch.randn((N, noise_dim, 1, 1))\n",
    "assert gen(z).shape == (N, in_channels, H, W), \"Generator test failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65a69f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.WGANx64 import Critic, Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "946fa3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = Critic()\n",
    "gen = Generator(noise_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "438978af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "680cf6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc(x).reshape(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1464d6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 64, 64])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(z).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b685cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([98304])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(z).reshape(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6582f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
